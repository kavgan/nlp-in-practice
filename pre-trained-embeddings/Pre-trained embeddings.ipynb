{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained embeddings and NLP corpora\n",
    "\n",
    "Gensim has some really nice functionality, in that it allows you to use pre-trained GloVe and Word2Vec embeddings with its libraries. In addition there are also some re-usable corpora that you can download and immediately use to train a Word2Vec embedding. The code snippets below show you how. The source of the embeddings can be found here: https://github.com/RaRe-Technologies/gensim-data.\n",
    "\n",
    "I'll have to warn you that I'm not impressed with the quality of the pre-trained word embeddings. Either the dataset is noisy or its just too general. To be explained more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained: Twitter GloVe Embeddings\n",
    "This first step downloads the pre-trained embeddings and loads it for re-use. Note that these are GloVe embeddings built using Tweets as the name suggests. These vectors are based on 2B tweets, 27B tokens, 1.2M vocab, uncased. The original source can be found here: https://nlp.stanford.edu/projects/glove/. The `25` in the model name refers to the dimensionality of the vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and return as object ready for use\n",
    "model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have loaded the pre-trained model, just use it as you would with any gensim word2vec model. Here are a few similarity examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('clegg', 0.9653650522232056),\n",
       " ('miliband', 0.9515050053596497),\n",
       " ('bachmann', 0.9484400153160095),\n",
       " ('mcconnell', 0.9416399002075195),\n",
       " ('carney', 0.9340256452560425),\n",
       " ('coulter', 0.9311323165893555),\n",
       " ('boehner', 0.9286301732063293),\n",
       " ('santorum', 0.9269059896469116),\n",
       " ('farage', 0.9193653464317322),\n",
       " ('mourdock', 0.9186689853668213)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('policy', 0.9484812021255493),\n",
       " ('reforms', 0.9403934478759766),\n",
       " ('laws', 0.9401204586029053),\n",
       " ('government', 0.9230710864067078),\n",
       " ('regulations', 0.9168934226036072),\n",
       " ('economy', 0.9110006093978882),\n",
       " ('immigration', 0.9105910062789917),\n",
       " ('legislation', 0.9089650511741638),\n",
       " ('govt', 0.9054746627807617),\n",
       " ('regulation', 0.9050778746604919)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"policies\",topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trainend: GloVe Wikipedia + Gigaword \n",
    "The example below uses pre-trained GloVe vectors based on Wikipedia 2014 and Gigaword. The original source of these embeddings can be found here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#again, download and load the model\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.7690386176109314),\n",
       " ('smelly', 0.7392696738243103),\n",
       " ('shabby', 0.7025482654571533),\n",
       " ('dingy', 0.7022336721420288),\n",
       " ('grubby', 0.6754513382911682),\n",
       " ('grungy', 0.6414024233818054),\n",
       " ('dank', 0.6263698935508728),\n",
       " ('sweaty', 0.622745156288147),\n",
       " ('dreary', 0.6216242909431458),\n",
       " ('gritty', 0.6215749382972717)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similarity\n",
    "model_gigaword.wv.most_similar(positive=['dirty','grimy'],topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spring', 0.8519278764724731),\n",
       " ('autumn', 0.7865706086158752),\n",
       " ('olympics', 0.6915045380592346),\n",
       " ('weekend', 0.6908971667289734),\n",
       " ('days', 0.6872981786727905),\n",
       " ('during', 0.6861999034881592),\n",
       " ('season', 0.6849778294563293),\n",
       " ('year', 0.6827663779258728),\n",
       " ('rainy', 0.6744828820228577),\n",
       " ('day', 0.671191930770874)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.wv.most_similar(positive=[\"summer\",\"winter\"],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a dataset and train a model\n",
    "Instead of loading pre-trained embeddings, you can also load a corpus and train it on demand. This list of datasets that you can download can be found here: https://github.com/RaRe-Technologies/gensim-data#datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# this loads the text8 dataset\n",
    "corpus = api.load('text8')\n",
    "\n",
    "# train a Word2Vec model\n",
    "model_text8 = Word2Vec(corpus,iter=10,size=150, window=10, min_count=2, workers=10)  # train a model from the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('surprised', 0.7146514058113098),\n",
       " ('outraged', 0.7117233276367188),\n",
       " ('disappointed', 0.6712729930877686),\n",
       " ('angered', 0.6455301642417908),\n",
       " ('offended', 0.6371268630027771),\n",
       " ('overwhelmed', 0.6347959637641907),\n",
       " ('confronted', 0.6278891563415527),\n",
       " ('betrayed', 0.6236147284507751),\n",
       " ('disgusted', 0.6220308542251587),\n",
       " ('alarmed', 0.6148042678833008)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity \n",
    "model_text8.wv.most_similar(\"shocked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44690064"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model_text8.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model_text8.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
